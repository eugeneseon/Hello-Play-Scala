//db.default.driver = "com.mysql.jdbc.Driver",
db.default.driver = "org.mariadb.jdbc.Driver",
db.default.url="jdbc:mysql://localhost:3306/figaro"
db.default.user="root"
db.default.password="password" 

#applyEvolutions.default=true


# Set a connection's default autocommit setting
db.default.autocommit=true

# Set a connection's default isolation level
db.default.isolation=READ_COMMITTED

# In order to reduce lock contention and thus improve performance, 
# each incoming connection request picks off a connection from a 
# pool that has thread-affinity. 
# The higher this number, the better your performance will be for the 
# case when you have plenty of short-lived threads. 
# Beyond a certain threshold, maintenance of these pools will start 
# to have a negative effect on performance (and only for the case 
# when connections on a partition start running out).
db.default.partitionCount=2

# The number of connections to create per partition. Setting this to 
# 5 with 3 partitions means you will have 15 unique connections to the 
# database. Note that BoneCP will not create all these connections in 
# one go but rather start off with minConnectionsPerPartition and 
# gradually increase connections as required.
db.default.maxConnectionsPerPartition=5

# The number of initial connections, per partition.
db.default.minConnectionsPerPartition=5

# When the available connections are about to run out, BoneCP will 
# dynamically create new ones in batches. This property controls 
# how many new connections to create in one go (up to a maximum of 
# maxConnectionsPerPartition). Note: This is a per-partition setting.
db.default.acquireIncrement=1

# After attempting to acquire a connection and failing, try to 
# connect this number of times before giving up.
db.default.acquireRetryAttempts=10

# How long to wait before attempting to obtain a 
# connection again after a failure.
db.default.acquireRetryDelay=5 seconds

# The maximum time to wait before a call 
# to getConnection is timed out.
db.default.connectionTimeout=1 second

# Idle max age
db.default.idleMaxAge=10 minute

# This sets the time for a connection to remain idle before sending a test query to the DB. 
# This is useful to prevent a DB from timing out connections on its end. 
db.default.idleConnectionTestPeriod=5 minutes

# An initial SQL statement that is run only when 
# a connection is first created.
db.default.initSQL="SELECT 1"

# If enabled, log SQL statements being executed.
db.default.logStatements=false

# The maximum connection age.
db.default.maxConnectionAge=1 hour

# The maximum query execution time. Queries slower than this will be logged as a warning.
db.default.queryExecuteTimeLimit=1 second
 
# Slick Evolutions
# ~~~~~


# Hikaricp settings
#dbplugin=disabled
#db.default.hikaricp.file="conf/hikaricp.prod.properties"
ehcacheplugin=disabled



# Evolutions
# ~~~~~
# You can disable evolutions if needed
evolutionplugin=disabled

smtpserver-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 4.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 10
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 1
}  

akka.persistence.journal.plugin = "hbase-journal"

akka {

  loglevel = "INFO"
  log-dead-letters = on

  persistence {
    journal.plugin = "hbase-journal"

    #snapshot-store.plugin = "hadoop-snapshot-store"

    # we need event publishing for tests
    publish-confirmations = on
    publish-plugin-commands = on

    # disable leveldb (default store impl)
    journal.leveldb.native = off
  }

  log-dead-letters = 10
  log-dead-letters-during-shutdown = on
}


hbase-journal {
  publish-testing-events = on

  hadoop-pass-through {
    hbase.zookeeper.quorum = "freeneoweb1.c.absolute-runner-550.internal:2181,freeneoweb2.c.absolute-runner-550.internal:2181,freeneoweb3.c.absolute-runner-550.internal:2181"
     zookeeper.znode.parent = "/hbase-unsecure"
  }
}


hadoop-snapshot-store {
  # class name of snapshot plugin
  class = "akka.persistence.hbase.snapshot.HadoopSnapshotStore"

  # select your preferred implementation based on your needs
  #
  # * HBase - snapshots stored together with snapshots; Snapshot size limited by Int.MaxValue bytes (currently)
  #   impl class is "akka.persistence.hbase.snapshot.HBaseSnapshotter"
  #
  # * HDFS *deprecated, will be separate project* - can handle HUGE snapshots;
  #          Can be easily dumped to local filesystem using Hadoop CL tools
  #          impl class is "akka.persistence.hbase.snapshot.HdfsSnapshotter"
  mode = "hbase"

  hbase {
    # Name of the table to be used by the journal
    table = "akka_snapshots"

    # Name of the family to be used by the journal
    family = "snapshot"
  }

  # directory on HDFS where to store snapshots
  snapshot-dir = "/akka-persistence-snapshots"
}